# Shobana-project
Microservices/Kubernetes Udacity Project
This microservices application is built using flask app and postgresql DB. It consists of 2 microservices - 1 for app and 1 for DB. 
Prerequisitesto deploy the application - setup an EKS Cluster with 2 nodegroups created(setting up IAM roles and attach policies). Referred to AWS documentation for Role and polcies required for Cluster and node groups.
Set up AWS CodeBuild - dockerfile enclosed which is used to build the image using codebuild pipeline and pushed to ECR Repo. The OS I have used for the codebuild is Amazon Linux, Standard Runtime, AL2 aarch64-std 2.0 Image. I have input image repo name and image tag as the variables (image tag with codebuildnumber to increment automatically with the code build number and semantic version applied). The buildspec yaml file is enclosed here in this repo which details the build steps. 
Docker Image - Dockerfile is enclosed in this repo which is simple few steps to run the 2 pythin files - config.py and app.py. Dependencies are specified within requirements.txt. The codebuild automated pipeline will push the image built to the repo that is specified within the codebuild comnfiguration(as env variable).
Deploying with Kubernetes - Kubectl apply the depl-success.yml file to deploy the app container. Prior to applying the depl-success.yml file, the env-config.yml and secrets-config.yml files have to be applied as they hold the environment variables such as db_username, db_password, db_name, db_port and db_host. The sensitive parameters of db_username and db_pasword are in secrets-config.yml. These are configmap files. The values here can be modified according to each new deployment and these files applied prior to deploying the app. 
CloudWatch Logs - EKS cluster configured to send logs to cloudwatch by setting up FluentBit(Referred to AWS documentation to set up). Logs show successful json outputs of sucessful run of the containerized app.
The postgresql DB was installed using helm. dbusername(postgres by default) and dbpassword can be echoed using commands. Connection -(forward port) connection via 5432 port was achieved and added all DB files from local machine to the postgresql pod. 
The endpoints used to check for successful working of the app is /api/reports/daily_usage and /api/reports/user_visits. This output the json format of the daily_usage table and user_visits table. I have used load balancer as the port for the app service as that will allow me to access from outside the kubernetes. 
For the EKS Cluster I used AL2 ARM 64 AMI and m6large and 20GiB Disk Size and 2 worker nodes. However this can be adjusted according to use case and workloads and I would look to autoscale the deployment through monitoring CPU usage level and if below certain threshold , will scale down the number of worker nodes and vice versa. I would do the same to reduce CPU and memory if workloads are less and not using the limits available. This is something that will be monitored through cloudwatch and automated for autoscaling number of nodes, cpu and memory.
The role-binding-configmap.yml and role-configmap.yml files have the neccessary permissions for Cluster to read information from the configmap files. These 2 files were applied prior to deploying the app so the container will ahve the correct environment varaibake sread from the configmap files. 
The db_host information was obtained from the output of the describe service postgresql and this is input in the env-config file. 
All the above steps will deploy the containerised app successfully and outpiuts can be checked via the endpoints and cloudwatch can be used to monitor application logs. 
